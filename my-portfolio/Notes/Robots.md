# What is Robots?

A robots.txt file is a set of instructions for bots.
A robots. txt file tells search engine crawlers which URLs the crawler can access on your site.
This is used mainly to avoid overloading your site with requests; it is not a mechanism for keeping a web page out of Google.
To keep a web page out of Google, block indexing with noindex or password-protect the page.

# What is a web crawler bot?

A web crawler, spider, or search engine bot downloads and indexes content from all over the Internet.
The goal of such a bot is to learn what (almost) every webpage on the web is about, so that the information can be retrieved when it's needed
